# Simulated Exposure Review Tool

User can upload JSON files generated by an assessment tool and the application will review them and keep a record of
the data. The findings and recommended actions will be available to the viewers of the report.

# Building & Running

The application is set up to use `docker compose` so Docker CE or equivalent are needed, optionally with the compose plugin.

To build it:
```sh
docker compose build 
```

To run it:
```sh
docker compose run --wait
```

By default, the app is hosted on port `8000`. This may be changed by modifying the following files and changing `8000` to
something else:
* `Dockerfile`
* `docker-compose.yml`
* `exposure-front/nginx.conf`

To verify execution, after it starts up `docker compose logs` can show the system log, which should look something
like this (with no errors reported by `nginx`):
```
exposure-tool-1  | [2025-01-26 15:47:07 +0000] [10] [INFO] Starting gunicorn 23.0.0
exposure-tool-1  | [2025-01-26 15:47:07 +0000] [10] [INFO] Listening at: http://127.0.0.1:8001 (10)
exposure-tool-1  | [2025-01-26 15:47:07 +0000] [10] [INFO] Using worker: sync
exposure-tool-1  | [2025-01-26 15:47:07 +0000] [11] [INFO] Booting worker with pid: 11
exposure-tool-1  | [2025-01-26 15:47:07 +0000] [12] [INFO] Booting worker with pid: 12
exposure-tool-1  | [2025-01-26 15:47:07 +0000] [13] [INFO] Booting worker with pid: 13
exposure-tool-1  | [2025-01-26 15:47:07 +0000] [14] [INFO] Booting worker with pid: 14
```

Note that, internally to the container, the API is listening on port `8001` and the `nginx` process running on port `8000`
routes `/api/` traffic to it.

## Running without Docker Compose

It can be run without `docker compose`. In that event, to build:

```sh
docker build -t exposure-tool:latest .
```

And then to run (example below keeps it local and iteractive so ^C can easily shut it down):
```sh
docker run -p 8000:8000 -it exposure-tool:latest
```

# Accessing 

Once running, the application can be accessed via a web browser at `http://localhost:8000`, or via the IP address of the host system on port `8000`. The
`docker-compose.yml` is written such that the application will use the local folder `reports/` for storing persistent information, so data will be preserved
across restarts, and image rebuilds. If you want to remove some or all of the reports, delete contents from the `reports/` folder.

# Using

Initial view of the application displays a list of the recent reports. If no reports have been uploaded, the list will be empty.

![Entry Page of App](/screenshots/home.png)

Clicking on the "UPLOAD NEW ASSESSMENT FILE" button will allow you to upload new assessments into the tool, giving them a short descriptive name

![Upload Assessment](/screenshots/upload.png)

After uploading, a message will be displayed above the button that indicates whether the assessment was uploaded successfully, or if there was some
error.

Clicking on one of the assessments in the list will display a report of the defects (and remediation recommendations, if any) for the assessed infrastructure.

![Report](/screenshots/report.png)

# Notes

- Application is written with a Python Flask back-end API, and a client-side React app for the UI
- Listing of assessments is done by just showing the most recently-timestamped 10 - a fuller solution would use pagination implemented in the front and back-end
- `nginx` handles routing inbound traffic either to the `gunicorn` server running the Flask-based API or serving the static content of the React UI
- No authentication or user management is implemented, but it would make sense for the data set to be access-controlled by user account and groups
- The application compares provided infrastructure JSON against a policy baseline and displays deviations from that base as "defects"
- Back-end storage is a simple file tree, rather than a database. The current setup lends itself to adapting to an object store, which could then utilize data lake tech to achieve more complex analysis and reporting.
- The application doesn't provide a means for lifecycle tracking of findings, but that would be a valuable addition to it
- Expectation would be that a single user, or small team of users, would use this internally for pilot use, with the goal of usage driving more requirements

# Policies

There are 3 policy types defined, based upon the 3 types of infrastructure available in the demonstration data set:
* Database
* Virtual Machine
* Storage Account

Each policy is written as Python code in [`/exposure-back/policies/policy_checker.py`]. For scalable future expansion, the policy constraints defined
in there should be parsed from pulished structured policy specification documents. This would allow some external team/party to maintain the policies,
and this tool would be able to regularly consume that dataset and then update assessments accordingly.

This implementation simulates a handful of compliance requirements based upon some common "best practices" that I've gathered along the way, in order to
demonstrate the action of grading a collection of varying infrastructure against a common checklist of policy requirements. The purpose would be to focus
conversation on critical policy deviations.
